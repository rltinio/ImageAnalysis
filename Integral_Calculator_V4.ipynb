{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import ast\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import find_peaks\n",
    "from Tusc5IntegralUtils import *\n",
    "from Tusc5ImageUtils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Accepts data from the new image analysis\n",
    "'''\n",
    "\n",
    "raw_data_csv_name = # input\n",
    "Cell_Results = pd.read_csv('raw_data_folder/' + raw_data_csv_name + '.csv')\n",
    "Cell_Results['X_vals'] = Cell_Results['X_vals'].apply(ast.literal_eval) # Makes it so cells containing lists are identifiable as lists by pandas\n",
    "Cell_Results['Y_vals'] = Cell_Results['Y_vals'].apply(ast.literal_eval)\n",
    "Cell_Results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DJID_GEN_Eye Merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "'DJID_GEN_EYES.csv' contains DJIDs which connect DJIDs to genotype and which eye was in the experimental or control group\n",
    "\n",
    "This code chunk merges that dataframe with Cell_Results\n",
    "\n",
    "\n",
    "Takes into consideration if file_name has variations on eye types (e.g. RA, LD)\n",
    "'''\n",
    "\n",
    "Recorded_Info = pd.read_csv('DJID_GEN_EYES_DURATION.csv')\n",
    "merged_df = pd.merge(Cell_Results, Recorded_Info[['DJID', 'Genotype', 'Experimental Eye', 'Control Eye', 'Time_Condition']], on='DJID', how='left')\n",
    "\n",
    "merged_df.loc[merged_df['Eye'] == merged_df['Experimental Eye'], 'Group_Type'] = 'Experimental'\n",
    "merged_df.loc[merged_df['Eye'] == merged_df['Control Eye'], 'Group_Type'] = 'Control'\n",
    "\n",
    "# Special case Group_Type assignment for time_series_old_batch_new\n",
    "merged_df.loc[merged_df['Eye'].str[-1].isin(['A', 'B']), 'Group_Type'] = 'Experimental'\n",
    "merged_df.loc[merged_df['Eye'].str[-1].isin(['C', 'D']), 'Group_Type'] = 'Control'\n",
    "\n",
    "\n",
    "Cell_Results = merged_df.drop(columns=['Experimental Eye', 'Control Eye'])\n",
    "\n",
    "if sum(merged_df['Group_Type'].isna()) > 0:\n",
    "    print('WARNING: There is an NaN value in the Group_Type column. Check the file_name and DJID_GEN_DURATION.csv for errors')\n",
    "\n",
    "Cell_Results.loc[Cell_Results['Genotype'] == 'wildtype', 'eGFP_Value'] = False # Making sure wildtype retinas do not have false positive eGFP cells\n",
    "\n",
    "Cell_Results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('STATS BEFORE FILTER')\n",
    "\n",
    "print('#'*50)\n",
    "print('DJID:', Cell_Results.DJID.unique())\n",
    "print('in_rip:', Cell_Results.in_rip.unique())\n",
    "print('Group_type:', Cell_Results.Group_Type.unique())\n",
    "print('Time Conditions:', Cell_Results.Time_Condition.unique())\n",
    "print('Genotype:', Cell_Results.Genotype.unique())\n",
    "print('#'*50)\n",
    "\n",
    "print(Cell_Results.groupby('DJID')['Time_Condition'].count()/4)\n",
    "print('#'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Birth Data Merge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Fetching a table to connect DJID to mouse age\n",
    "Then merging that table to Cell_Results\n",
    "\n",
    "'''\n",
    "\n",
    "# DJ query and fetch\n",
    "init_query = sln_animal.Animal.proj('dob', 'sex') * sln_animal.AnimalEvent * sln_animal.Deceased\n",
    "birth_data = init_query.proj('dob', 'sex', 'date').fetch(format = 'frame')\n",
    "\n",
    "# Reseting index and renaming columns\n",
    "birth_data = birth_data.reset_index().drop(columns = ['event_id'])\n",
    "birth_data = birth_data.rename(columns = {'animal_id': 'DJID', 'dob': 'birth', 'date': 'death'})\n",
    "\n",
    "# Changing variables to date time vars\n",
    "birth_data['birth'] = pd.to_datetime(birth_data['birth'])\n",
    "birth_data['death'] = pd.to_datetime(birth_data['death'])\n",
    "\n",
    "# Calculate the difference in days\n",
    "birth_data['Age_Days'] = (birth_data['death'] - birth_data['birth']).dt.days\n",
    "\n",
    "# Convert days to months (approximately, assuming ~30.44 days in a month)\n",
    "birth_data['Age_Months'] = round(birth_data['Age_Days'] / 30.44,1)\n",
    "\n",
    "# Merge\n",
    "Cell_Results = Cell_Results.merge(birth_data, how = 'left', on = 'DJID')\n",
    "Cell_Results = Cell_Results.drop(columns = ['birth', 'death', 'Age_Days'])\n",
    "Cell_Results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slice Separation and ID generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "1) Adds a column to the dataframe which contains the z-stack slice seperation value\n",
    "\n",
    "2) Adds a cell id\n",
    "\n",
    "'''\n",
    "Cell_Results['Slice_Seperation'] = Cell_Results['X_vals'].apply(lambda x: x[1]-x[0])\n",
    "\n",
    "Cell_Results['Cell'] = Cell_Results.groupby(['file_name', 'mask_id']).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell_Results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_single_cell(Cell_Results.query('Cell == 474'), prominence= 1000, distance = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WGA_Peaks_Finder_V2(dataframe, prom_val: int = 1000):\n",
    "    '''\n",
    "    This function finds WGA peaks before and after the first DAPI peak in each cell.\n",
    "\n",
    "    It filters out cells with multiple DAPI peaks and leaves cells without a DAPI peak as NaN for the DAPI peak index.\n",
    "\n",
    "    Returns:\n",
    "    - 'WGA_Middle_Indices' as a tuple of (WGA peak before DAPI, WGA peak after DAPI), with None if a peak is not found.\n",
    "    - 'DAPI_peak_index' as NaN if no valid DAPI peak is found, or as a single integer if exactly one DAPI peak exists.\n",
    "    '''\n",
    "    \n",
    "    # Function to find WGA peaks for each cell (via .groupby)\n",
    "    def filter_peaks_in_cell(cell_df):\n",
    "        # Filter rows where Stain is 'WGA' and 'DAPI'\n",
    "        df_WGA = cell_df.loc[cell_df['Stain'] == 'WGA']\n",
    "        df_DAPI = cell_df.loc[cell_df['Stain'] == 'DAPI']\n",
    "\n",
    "        # Find slice separation\n",
    "        slice_separation = df_WGA.iloc[0]['Slice_Seperation']\n",
    "\n",
    "        # Set distance where only one DAPI peak exists\n",
    "        DAPI_min_distance = int(12 / slice_separation)\n",
    "        WGA_min_distance = int(1.05 / slice_separation) \n",
    "        \n",
    "        # Finding peaks for DAPI and WGA\n",
    "        DAPI_peak_indicies = find_peaks(df_DAPI['Y_vals'].iloc[0], prominence=prom_val, distance=DAPI_min_distance)[0]\n",
    "        WGA_peak_indicies = find_peaks(df_WGA['Y_vals'].iloc[0], prominence=prom_val, distance=WGA_min_distance)[0]\n",
    "\n",
    "        # Initialize variables for WGA peaks before and after DAPI peak\n",
    "        first_WGA_before_DAPI = np.NaN\n",
    "        first_WGA_after_DAPI = np.NaN\n",
    "\n",
    "        # If we have exactly one DAPI peak, proceed with finding WGA peaks before and after\n",
    "        if isinstance(DAPI_peak_indicies, (np.ndarray, list)) and len(DAPI_peak_indicies) == 1:\n",
    "            DAPI_peak_index = int(DAPI_peak_indicies[0])  # Convert to int explicitly to ensure it's an int\n",
    "\n",
    "            # Iterate over WGA peaks to find the first before and after the DAPI peak\n",
    "            for peak in WGA_peak_indicies:\n",
    "                if peak < DAPI_peak_index:\n",
    "                    first_WGA_before_DAPI = peak  # Update with the latest WGA peak before DAPI\n",
    "                elif peak > DAPI_peak_index and np.isnan(first_WGA_after_DAPI):\n",
    "                    first_WGA_after_DAPI = peak  # First WGA peak after DAPI, break the loop\n",
    "                    break\n",
    "        else:\n",
    "            # If there are no DAPI peaks or multiple DAPI peaks, set DAPI_peak_index to NaN\n",
    "            DAPI_peak_index = np.NaN\n",
    "\n",
    "        # Return both WGA peaks and the DAPI peak index (as an integer or NaN)\n",
    "        return [first_WGA_before_DAPI, first_WGA_after_DAPI], DAPI_peak_index\n",
    "\n",
    "    # Apply the peak finder function across cells grouped by 'Cell'\n",
    "    grouped = dataframe.groupby('Cell')\n",
    "    \n",
    "    # Apply the filtering function to each group of cells and unpack the results\n",
    "    grouped_results = grouped.apply(lambda cell: pd.Series(filter_peaks_in_cell(cell))).reset_index()\n",
    "    \n",
    "    # Rename the columns where we stored the list of WGA peaks and DAPI peaks\n",
    "    grouped_results = grouped_results.rename(columns={0: 'WGA_Middle_Indices', 1: 'DAPI_peak_index'})\n",
    "    \n",
    "    # Calculate the length between the two peaks and add a Length column\n",
    "    grouped_results['Length'] = grouped_results['WGA_Middle_Indices'].apply(\n",
    "        lambda x: (x[1] - x[0]) * dataframe['Slice_Seperation'].iloc[0] \n",
    "        if not np.isnan(x[0]) and not np.isnan(x[1]) else np.NaN\n",
    "    )\n",
    "\n",
    "    # Merge the peak indices and DAPI peaks back into the original DataFrame\n",
    "    result_df = pd.merge(dataframe, grouped_results[['Cell', 'WGA_Middle_Indices', 'DAPI_peak_index', 'Length']], on='Cell', how='left')\n",
    "\n",
    "    return result_df.copy()\n",
    "\n",
    "Cell_Results = WGA_Peaks_Finder_V2(Cell_Results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removed filter_out_bad_peaks function\n",
    "\n",
    "def filter_out_unclear_DAPI(dataframe):\n",
    "    '''\n",
    "    This function filters cells from a DataFrame based solely on the presence of a clear DAPI peak.\n",
    "    \n",
    "    Criteria:\n",
    "    1. The cell must have a clear DAPI peak, either as a single value or as the first element in a list.\n",
    "    \n",
    "    Parameters:\n",
    "    - dataframe: Input DataFrame with a 'DAPI_peak_index' column representing DAPI peak positions.\n",
    "\n",
    "    Returns:\n",
    "    - A filtered DataFrame retaining only cells with a clear DAPI peak.\n",
    "    '''\n",
    "    \n",
    "    # Helper function to check if there is a valid DAPI peak\n",
    "    def get_first_DAPI_peak(DAPI_peak):\n",
    "        if pd.isna(DAPI_peak):\n",
    "            return None  # No valid DAPI peak\n",
    "        else:\n",
    "            return DAPI_peak  # Single DAPI peak as an integer\n",
    "    \n",
    "    # Helper function to validate if a cell has a clear DAPI peak\n",
    "    def is_cell_valid(row):\n",
    "        DAPI_peak = get_first_DAPI_peak(row['DAPI_peak_index'])\n",
    "        return DAPI_peak is not None  # The cell is valid if there's a valid DAPI peak\n",
    "    \n",
    "    # Identify the invalid cells\n",
    "    invalid_cells_dataframe = dataframe[~dataframe.apply(is_cell_valid, axis=1)].reset_index(drop=True)\n",
    "    \n",
    "    # Print the IDs of the cells that are filtered out\n",
    "    if not invalid_cells_dataframe.empty:\n",
    "        print(\"Filtered out cells:\", invalid_cells_dataframe['Cell'].unique().tolist())\n",
    "    else:\n",
    "        print(\"No cells were filtered out.\")\n",
    "    \n",
    "    # Filter out the invalid cells and return the valid cells\n",
    "    filtered_cells_dataframe = dataframe[dataframe.apply(is_cell_valid, axis=1)].reset_index(drop=True)\n",
    "    \n",
    "    return filtered_cells_dataframe.copy()\n",
    "\n",
    "Cell_Results = filter_out_unclear_DAPI(Cell_Results)\n",
    "\n",
    "Cell_Results['DAPI_peak_index'] = pd.to_numeric(Cell_Results.DAPI_peak_index, downcast='integer') # Changing column to int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function: Returns cells with only one peak\n",
    "def filter_for_single_peak(dataframe):\n",
    "    '''\n",
    "    Returns:\n",
    "    - A filtered DataFrame retaining only cells with one WGA peak before the DAPI peak.\n",
    "    '''\n",
    "    \n",
    "    # Helper function to check if there is a valid DAPI peak\n",
    "    def get_first_DAPI_peak(DAPI_peak):\n",
    "        if isinstance(DAPI_peak, list) or isinstance(DAPI_peak, np.ndarray):\n",
    "            if len(DAPI_peak) > 0:\n",
    "                return DAPI_peak[0]  # If DAPI peaks are in a list, take the first one\n",
    "            else:\n",
    "                return None  # Empty list/array, no valid DAPI peak\n",
    "        elif pd.isna(DAPI_peak):\n",
    "            return None  # No valid DAPI peak\n",
    "        else:\n",
    "            return DAPI_peak  # Single DAPI peak as an integer\n",
    "    \n",
    "    # Helper function to validate if a cell meets the criteria\n",
    "    def is_cell_valid(row):\n",
    "        WGA_peaks = row['WGA_Middle_Indices']\n",
    "        DAPI_peak = get_first_DAPI_peak(row['DAPI_peak_index'])\n",
    "\n",
    "        # Ensure we have a valid DAPI peak\n",
    "        if DAPI_peak is None:\n",
    "            return False\n",
    "\n",
    "        # Check that there's exactly one valid WGA peak before the DAPI peak\n",
    "        WGA_before_DAPI = WGA_peaks[0]  # First WGA peak (before DAPI)\n",
    "        \n",
    "        if pd.isna(WGA_before_DAPI) or WGA_before_DAPI >= DAPI_peak:\n",
    "            return False  # There must be one WGA peak before the DAPI peak\n",
    "        \n",
    "        # The second WGA peak must be NaN, indicating there is only one peak\n",
    "        WGA_after_DAPI = WGA_peaks[1] if len(WGA_peaks) > 1 else np.NaN\n",
    "        if not pd.isna(WGA_after_DAPI):\n",
    "            return False  # If there's a second WGA peak, reject the cell\n",
    "\n",
    "        return True\n",
    "\n",
    "    # Filter out the invalid cells and return the valid cells\n",
    "    filtered_cells_dataframe = dataframe[dataframe.apply(is_cell_valid, axis=1)].reset_index(drop=True)\n",
    "    \n",
    "    return filtered_cells_dataframe\n",
    "\n",
    "#'''\n",
    "#Quick visualizations to see cells that have a single peak or in a rip\n",
    "#'''\n",
    "\n",
    "#example_frame = filter_for_single_peak(Cell_Results)\n",
    "#rand_cell = np.random.choice(example_frame.Cell.unique())\n",
    "#plot_single_cell(example_frame.query('Cell == @rand_cell'))\n",
    "\n",
    "#rand_cell = np.random.choice(Cell_Results.query('in_rip == True').Cell.unique())\n",
    "# plot_single_cell(Cell_Results.query('Cell == @rand_cell'), prominence = 1000, distance = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filters (Retired 9/10/24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Goes through each cell in Cell_Results and calculates peaks\n",
    "\n",
    "Prominence = 25 was chosen as it experimentally retained the most number of good cells.\n",
    "\n",
    "'''\n",
    "\n",
    "# Peak Filter\n",
    "\n",
    "print(f'Original number of Cells: {len(Cell_Results)/4}')\n",
    "\n",
    "remove_list = []\n",
    "\n",
    "for cell_number in Cell_Results['Cell'].unique():\n",
    "    \n",
    "    CR_Query = Cell_Results.query('Cell == @cell_number').copy()\n",
    "\n",
    "    slice_seperation = CR_Query.iloc[0]['Slice_Seperation']\n",
    "\n",
    "    WGA_min_distance = int(2.25 / slice_seperation) # peaks must be 2.25 microns away from eachother\n",
    "    DAPI_min_distance = int(10 / slice_seperation)\n",
    "    \n",
    "    index_max = len(CR_Query.iloc[0]['X_vals'])\n",
    "\n",
    "    WGA_Y_vals = CR_Query.loc[CR_Query['Stain'] == 'WGA']['Y_vals'].iloc[0]\n",
    "    DAPI_Y_vals = CR_Query.loc[CR_Query['Stain'] == 'DAPI']['Y_vals'].iloc[0]\n",
    "\n",
    "    peaks_WGA, _ = find_peaks(WGA_Y_vals, prominence= 25, distance = WGA_min_distance)\n",
    "    peaks_DAPI, _ = find_peaks(DAPI_Y_vals, prominence= 25, distance = DAPI_min_distance)\n",
    "\n",
    "    '''\n",
    "    WGA Filters\n",
    "\n",
    "    '''\n",
    "    \n",
    "    # 1) Remove if less than two peaks\n",
    "    if len(peaks_WGA) < 2:\n",
    "        remove_list.append(cell_number)\n",
    "        continue\n",
    "    \n",
    "\n",
    "    if peaks_WGA[1] != peaks_WGA[-1] and WGA_Y_vals[peaks_WGA[-1]] > WGA_Y_vals[peaks_WGA[1]]:\n",
    "        remove_list.append(cell_number)\n",
    "        continue\n",
    "\n",
    "    # '''\n",
    "    # DAPI Filters\n",
    "    # '''\n",
    "\n",
    "    if len(peaks_DAPI) < 1:\n",
    "        remove_list.append(cell_number)\n",
    "        continue\n",
    "\n",
    "    elif len(peaks_DAPI) == 1 and peaks_DAPI[0] > peaks_WGA[0] and peaks_DAPI[0] < peaks_WGA[1]:\n",
    "        pass\n",
    "\n",
    "    if peaks_DAPI[0] != peaks_DAPI[-1] and DAPI_Y_vals[peaks_DAPI[-1]] > DAPI_Y_vals[peaks_DAPI[1]]:\n",
    "        remove_list.append(cell_number)\n",
    "\n",
    "# Removing undesirable cells from main list\n",
    "# remove_list = np.unique(remove_list)\n",
    "# Cell_Results = Cell_Results[~Cell_Results['Cell'].isin(remove_list)]\n",
    "\n",
    "# print(f'Remaining Cells after Filter: {len(Cell_Results)/4}')\n",
    "# print(f'Cell Numbers Removed {remove_list}, count : {len(remove_list)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('STATS AFTER FILTER')\n",
    "\n",
    "print('#'*50)\n",
    "print('DJID:', Cell_Results.DJID.unique())\n",
    "print('in_rip:', Cell_Results.in_rip.unique())\n",
    "print('Group_type:', Cell_Results.Group_Type.unique())\n",
    "print('Time Conditions:', Cell_Results.Time_Condition.unique())\n",
    "print('Genotype:', Cell_Results.Genotype.unique())\n",
    "print('#'*50)\n",
    "print(Cell_Results.groupby('DJID')['Time_Condition'].count()/4)\n",
    "print('#'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eGFP Finder (Retired when Image Analysis V2 released)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # eGFP finder\n",
    "# eGFP_filtered = []\n",
    "# remove_from_eGFP = []\n",
    "\n",
    "# for cell_number in Cell_Results['Cell'].unique():\n",
    "\n",
    "#     CR_Query = Cell_Results.query('Cell == @cell_number').copy()\n",
    "\n",
    "#     slice_seperation = CR_Query.iloc[0]['Slice_Seperation']\n",
    "#     DAPI_min_distance = int(10 / slice_seperation)\n",
    "\n",
    "#     DAPI_idx, _ = find_peaks(CR_Query.loc[CR_Query['Stain'] == 'DAPI']['Y_vals'].iloc[0], prominence= 25, distance = DAPI_min_distance)\n",
    "#     DAPI_peak_val = np.array(CR_Query.loc[CR_Query['Stain'] == 'DAPI', 'Y_vals'].iloc[0])[DAPI_idx[0]]\n",
    "\n",
    "#     # eGFP peak needs to be at least half of DAPI peak to be considered eGFP positive\n",
    "#     eGFP_idx, _ = find_peaks(CR_Query.loc[CR_Query['Stain'] == 'eGFP']['Y_vals'].iloc[0], prominence= 100, distance = 10, height = int(DAPI_peak_val/2))\n",
    "#     eGFP_peak_val = np.array(CR_Query.loc[CR_Query['Stain'] == 'eGFP', 'Y_vals'].iloc[0])[eGFP_idx]\n",
    "\n",
    "#     WGA_idx, _ = find_peaks(CR_Query.loc[CR_Query['Stain'] == 'WGA']['Y_vals'].iloc[0], prominence= 25)\n",
    "#     WGA_peak_val = np.array(CR_Query.loc[CR_Query['Stain'] == 'WGA', 'Y_vals'].iloc[0])[WGA_idx]\n",
    "\n",
    "#     if len(eGFP_peak_val) == 1:\n",
    "\n",
    "#         # If eGFP peak is not within WGA peaks, cell is not considered eGFP positive\n",
    "#         if eGFP_idx < WGA_idx[0] or eGFP_idx > WGA_idx[1]:\n",
    "#             remove_from_eGFP.append(cell_number)\n",
    "        \n",
    "#         # If eGFP is not at least 1/2 the size of \n",
    "#         if eGFP_peak_val > WGA_peak_val[1]/2:\n",
    "#             eGFP_filtered.append(cell_number)\n",
    "\n",
    "\n",
    "# eGFP_filtered = list(set(eGFP_filtered))\n",
    "# eGFP_filtered = sorted([i for i in eGFP_filtered if i not in remove_from_eGFP])\n",
    "\n",
    "# print(f'Cells with eGFP: {eGFP_filtered}, count: {len(eGFP_filtered)}')\n",
    "\n",
    "# # Labeling Cells T5 Positive or Negative\n",
    "# Cell_Results['T5'] = 'Negative'\n",
    "# Cell_Results.loc[Cell_Results['Cell'].isin(eGFP_filtered), 'T5'] = 'Positive'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comparison between\n",
    "# positive_old_identifier = np.array(eGFP_filtered)\n",
    "# positive_new_identifier = Cell_Results.loc[Cell_Results['eGFP_Value'] == True].Cell.unique()\n",
    "# print('Number of cells in old eGFP finder', len(positive_old_identifier))\n",
    "# print('Number of cells in new eGFP finder', len(positive_new_identifier))\n",
    "# print('Percent of old cells from old eGFP finder in new eGFP finder:', np.isin(positive_old_identifier, positive_new_identifier).sum()/ len(np.isin(positive_old_identifier, positive_new_identifier)) * 100, '%')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary of Cell_Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Sanity Check\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "print('Genotype', Cell_Results['Genotype'].unique())\n",
    "print('T5 Labelling', Cell_Results['Stain'].unique())\n",
    "print('eGFP Types', Cell_Results['eGFP_Value'].unique())\n",
    "\n",
    "print('\\n')\n",
    "print('###'*12)\n",
    "print('\\n')\n",
    "\n",
    "'''\n",
    "Summary of mouse information\n",
    "'''\n",
    "\n",
    "def extract_before_underscore(input_string):\n",
    "    parts = input_string.split(\"_\", 1)\n",
    "    if len(parts) > 1:\n",
    "        return parts[0]\n",
    "    \n",
    "eyes = Cell_Results['file_name'].apply(extract_before_underscore).unique()\n",
    "\n",
    "def extract_first_four_integers(input_string):\n",
    "    # Find all substrings of consecutive digits\n",
    "    integers = re.findall(r'\\d+', input_string)\n",
    "    \n",
    "    # Convert the found substrings into integers\n",
    "    integers = [int(i) for i in integers]\n",
    "    \n",
    "    # Return the first four integers\n",
    "    return integers[:4][0]\n",
    "\n",
    "animals = list(map(extract_first_four_integers, eyes))\n",
    "animals = pd.Series(animals).unique() # Extracting the unqiue values\n",
    "\n",
    "left = 0\n",
    "right = 0\n",
    "\n",
    "for i in eyes:\n",
    "    if 'R' in i:\n",
    "        right += 1\n",
    "    if 'L' in i:\n",
    "        left +=1\n",
    "\n",
    "positive_count = Cell_Results['eGFP_Value'].value_counts()[True]/4\n",
    "total = (len(Cell_Results))/4\n",
    "\n",
    "\n",
    "###\n",
    "print(f'Number of mice: {len(animals)}')\n",
    "print(f'Mice DJIDs: {animals}')\n",
    "print(f'Number of eyes: {len(eyes)}, Left = {left} R = {right}')\n",
    "\n",
    "###\n",
    "print('\\n')\n",
    "print('###'*12)\n",
    "print('\\n')\n",
    "\n",
    "###\n",
    "print('Number of Cells', Cell_Results['Cell'].count()/4)\n",
    "print('eGFP Positive', (Cell_Results.groupby('eGFP_Value').size()/4)[True])\n",
    "print('eGFP Negative', (Cell_Results.groupby('eGFP_Value').size()/4)[False])\n",
    "print('Positive over Total Percentage', positive_count/total*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell_Results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell_Results.groupby(['Genotype','DJID','sex'])['Age_Months'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RETIRED\n",
    "# WGA_Peaks_Finder is retired for WGA_Peaks_Finder_V2\n",
    "\n",
    "'''\n",
    "Function: Calculating WGA Peaks\n",
    "\n",
    "Creates columns: WGA_Middle_Indices, Length (um)\n",
    "\n",
    "'''\n",
    "\n",
    "def WGA_Peaks_Finder(dataframe, prom_val:int = 25):\n",
    "\n",
    "    # Function to find WGA peaks for each cell (via .groupby)\n",
    "    def find_peaks_in_cell(cell_df):\n",
    "        # Filter rows where Stain is 'WGA'\n",
    "        df_WGA = cell_df.loc[cell_df['Stain'] == 'WGA']\n",
    "        \n",
    "        slice_seperation = df_WGA.iloc[0]['Slice_Seperation']\n",
    "        WGA_min_distance = int(2.25 / slice_seperation) \n",
    "        \n",
    "        WGA_peak_indicies = find_peaks(df_WGA['Y_vals'].iloc[0], prominence = prom_val, distance = WGA_min_distance)[0]\n",
    "\n",
    "\n",
    "        return WGA_peak_indicies\n",
    "\n",
    "    # Apply the peak finder function across cells grouped by 'Cell'\n",
    "    grouped = dataframe.groupby('Cell')\n",
    "    slice_separation = grouped['Slice_Seperation'].first().iloc[0]\n",
    "\n",
    "    grouped_WGA_indices = grouped.apply(find_peaks_in_cell).reset_index().rename(columns={0: 'WGA_Middle_Indices'})    # Reset index to flatten the DataFrame and rename\n",
    "\n",
    "\n",
    "\n",
    "    grouped_WGA_indices['Length'] = grouped_WGA_indices['WGA_Middle_Indices'].apply(lambda x: (x[1] - x[0]) * slice_separation)\n",
    "\n",
    "    #Merge the peak indices back into the original DataFrame\n",
    "    result_df = pd.merge(dataframe, grouped_WGA_indices, on='Cell', how='left')\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function: Calculates \"Top\" and \"Bottom\" INDICES to be used for integral boundaries\n",
    "\n",
    "Creates columns: WGA_Top_Indices, WGA_Bottom_Indices\n",
    "'''\n",
    "\n",
    "def Top_Bottom_Indices_V2(dataframe, microns_extension: float = 1.5):\n",
    "    '''\n",
    "    Function: Calculates \"Top\" and \"Bottom\" INDICES to be used for integral boundaries.\n",
    "\n",
    "    V2: IF the bottom WGA is nan, returns None for bottom indicies \n",
    "    \n",
    "    Creates columns: WGA_Top_Indices, WGA_Bottom_Indices\n",
    "    '''\n",
    "    \n",
    "    # Grouping / Getting Slice Separation / Calculating peaks\n",
    "    grouped = dataframe.groupby('Cell')\n",
    "    slice_separation = grouped['Slice_Seperation'].first()\n",
    "    max_idx = grouped['X_vals'].apply(lambda x: len(x.iloc[0]))  # Length of each cell's X_vals\n",
    "    \n",
    "    first_peaks = grouped['WGA_Middle_Indices'].apply(lambda x: x.iloc[0] if len(x) > 0 else [np.nan, np.nan])\n",
    "\n",
    "    # Calculate index offset for each cell based on the slice separation\n",
    "    index_offset = (microns_extension / slice_separation).fillna(0).astype(int)\n",
    "\n",
    "    # Separate the peak indices\n",
    "    l_middle = first_peaks.apply(lambda x: x[0] if len(x) > 0 else np.nan)\n",
    "    r_middle = first_peaks.apply(lambda x: x[1] if len(x) > 1 else np.nan)\n",
    "\n",
    "    # Calculate modified indices\n",
    "    l_top = np.maximum(l_middle - index_offset, 0)  # Ensuring indices do not go below 0\n",
    "    r_bot = r_middle + index_offset\n",
    "\n",
    "    # If the bottom WGA peak is not found, set r_middle and r_bot to None\n",
    "    r_middle = r_middle.apply(lambda x: None if pd.isna(x) else x)\n",
    "    r_bot = r_bot.apply(lambda x: None if pd.isna(x) else x)\n",
    "\n",
    "    # Creating DataFrame for merging\n",
    "    idx_df = pd.DataFrame({\n",
    "        'Cell': grouped.size().index,  # Assures alignment with group keys\n",
    "        'WGA_Top_Indices': list(zip(l_top, l_middle)),\n",
    "        'WGA_Bottom_Indices': list(zip(r_middle, r_bot))\n",
    "    })\n",
    "\n",
    "    # Merge adjusted index data back into the original DataFrame\n",
    "    result_df = pd.merge(dataframe, idx_df, on='Cell', how='left')\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def TopMidBot_Integrals_V2(dataframe):\n",
    "    '''\n",
    "    Function: Calculates \"Top\", \"Middle\", \"Bottom\" INTEGRALS\n",
    "\n",
    "    V2: Returns None for Middle and Top integrals if right WGA peak is nan\n",
    "\n",
    "    Creates columns: Middle_Integral, Top_Integral, Bottom_Integral\n",
    "    '''\n",
    "    \n",
    "    # Allows you to calculate integrals through .apply()\n",
    "    def integral_calculator(y_vals, indices, integral_section: str = 'Middle'):\n",
    "        \"\"\"\n",
    "        This function calculates the sum of y-values within a specified range (top, middle, or bottom).\n",
    "        If the indices are None or invalid, it returns None.\n",
    "        \"\"\"\n",
    "        # Check if indices are None or contain NaN values\n",
    "        if indices is None or pd.isna(indices[0]) or pd.isna(indices[1]):\n",
    "            return None\n",
    "        \n",
    "        # Ensure the indices are integers (or convert them to integers)\n",
    "        try:\n",
    "            start_idx = int(indices[0])\n",
    "            end_idx = int(indices[1])\n",
    "        except (ValueError, TypeError):\n",
    "            return None\n",
    "        \n",
    "        # Adjust indices to stay within bounds\n",
    "        start_idx = max(start_idx, 0)  # If start index is less than 0, set to 0\n",
    "        end_idx = min(end_idx, len(y_vals))  # If end index is greater than length of y_vals, set to max length\n",
    "\n",
    "        # Ensure the range makes sense (start_idx should not be greater than end_idx)\n",
    "        if start_idx >= end_idx:\n",
    "            return None\n",
    "        \n",
    "        # Otherwise, safely slice the array using the adjusted indices\n",
    "        return np.sum(np.array(y_vals)[start_idx:end_idx])\n",
    "\n",
    "    # Iterate over 'Middle', 'Top', and 'Bottom' sections to compute integrals for each\n",
    "    for section in ['Middle', 'Top', 'Bottom']:\n",
    "        # Use apply to calculate the integrals and store them in new columns\n",
    "        dataframe[f'{section}_Integral'] = dataframe.apply(\n",
    "            lambda x: integral_calculator(x['Y_vals'], x[f'WGA_{section}_Indices'], integral_section=section), \n",
    "            axis=1\n",
    "        )\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Surface_Integrals_V2(dataframe):\n",
    "    '''\n",
    "    Function: Calculates \"Top\", \"Bottom\" INTEGRAL RATIOS\n",
    "\n",
    "    V2: Returns None for bottom surface if right WGA peak isn't found\n",
    "\n",
    "    # Calculating the ratio of Glut1 over WGA (normalization) 0.5 microns about the WGA peaks\n",
    "\n",
    "    Creates columns: [WGA or GluT1]_[Top or Bot]_Surface_Integral\n",
    "    '''\n",
    "\n",
    "    # Initialize a list to store results dataframes for each group\n",
    "    results = []\n",
    "\n",
    "    # Group by 'Cell' and process each group\n",
    "    for (cell, group) in dataframe.groupby('Cell'):\n",
    "        # Filter for WGA and GluT1 stains\n",
    "        df_WGA = group.loc[group['Stain'] == 'WGA']\n",
    "        df_GluT1 = group.loc[group['Stain'] == 'GluT1']\n",
    "\n",
    "        peak_indices = df_WGA['WGA_Middle_Indices'].iloc[0]\n",
    "        x_vals = df_WGA['X_vals'].iloc[0]\n",
    "\n",
    "        # Compute slice separation and radius offset\n",
    "        slice_separation = group['Slice_Seperation'].iloc[0]\n",
    "        radius = 0.5\n",
    "        idx_offset = int(radius / slice_separation)  # Ensure idx_offset is an integer\n",
    "\n",
    "        # Define borders based on peak indices and radius offset, with checks for missing peaks\n",
    "        if not pd.isna(peak_indices[0]):  # Check if top peak is valid\n",
    "            top_lborder = max(int(peak_indices[0]) - idx_offset, 0)  # Ensure integer indices\n",
    "            top_rborder = min(int(peak_indices[0]) + idx_offset, len(x_vals))\n",
    "        else:\n",
    "            top_lborder = None\n",
    "            top_rborder = None\n",
    "\n",
    "        # Bottom indices check (return None if the bottom peak is not found)\n",
    "        if not pd.isna(peak_indices[1]):  # Check if bottom peak is valid\n",
    "            bottom_lborder = max(int(peak_indices[1]) - idx_offset, 0)  # Ensure integer indices\n",
    "            bottom_rborder = min(int(peak_indices[1]) + idx_offset, len(x_vals))\n",
    "        else:\n",
    "            bottom_lborder = None\n",
    "            bottom_rborder = None\n",
    "\n",
    "        # Calculate integrals for 'GluT1' and 'WGA'\n",
    "        if top_lborder is not None and top_rborder is not None:\n",
    "            top_surface_integral_GluT1 = np.sum(df_GluT1.iloc[0]['Y_vals'][top_lborder:top_rborder])\n",
    "            top_surface_integral_WGA = np.sum(df_WGA.iloc[0]['Y_vals'][top_lborder:top_rborder])\n",
    "        else:\n",
    "            top_surface_integral_GluT1 = None\n",
    "            top_surface_integral_WGA = None\n",
    "\n",
    "        if bottom_lborder is not None and bottom_rborder is not None:\n",
    "            bottom_surface_integral_GluT1 = np.sum(df_GluT1.iloc[0]['Y_vals'][bottom_lborder:bottom_rborder])\n",
    "            bottom_surface_integral_WGA = np.sum(df_WGA.iloc[0]['Y_vals'][bottom_lborder:bottom_rborder])\n",
    "        else:\n",
    "            bottom_surface_integral_GluT1 = None\n",
    "            bottom_surface_integral_WGA = None\n",
    "\n",
    "        # Ratio calculations with checks for division by zero\n",
    "        Top_G_over_W = top_surface_integral_GluT1 / top_surface_integral_WGA if top_surface_integral_WGA and top_surface_integral_WGA != 0 else None\n",
    "        Bottom_G_over_W = (\n",
    "            bottom_surface_integral_GluT1 / bottom_surface_integral_WGA \n",
    "            if bottom_surface_integral_WGA and bottom_surface_integral_WGA != 0 else None\n",
    "        )\n",
    "\n",
    "        # Append results as a DataFrame to list\n",
    "        results.append(pd.DataFrame({\n",
    "            'Cell': [cell],\n",
    "            'GluT1_Top_Surface_Integral': [top_surface_integral_GluT1],\n",
    "            'GluT1_Bot_Surface_Integral': [bottom_surface_integral_GluT1],\n",
    "            'WGA_Top_Surface_Integral': [top_surface_integral_WGA],\n",
    "            'WGA_Bot_Surface_Integral': [bottom_surface_integral_WGA],\n",
    "            'Top_Surface_Ratio': [Top_G_over_W],\n",
    "            'Bot_Surface_Ratio': [Bottom_G_over_W]\n",
    "        }))\n",
    "\n",
    "    # Concatenate all results into a single DataFrame\n",
    "    results_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    # Merge adjusted index data back into the original DataFrame\n",
    "    dataframe = dataframe.merge(results_df, on='Cell', how='left')\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Function: Reshapes columns, so that each group has a column with '{Stain}_{Integral_Type}'. 4 x 3 = 12 columns are added\n",
    "\"\"\"\n",
    "\n",
    "def Reshape_Integrals(dataframe):\n",
    "\n",
    "    # Make pivot table\n",
    "    pivot_df = dataframe.pivot_table(\n",
    "        index=['Cell'], \n",
    "        columns='Stain',\n",
    "        values=['Middle_Integral', 'Top_Integral', 'Bottom_Integral'],\n",
    "        aggfunc='first' \n",
    "    )\n",
    "\n",
    "    # Flatten the multi-level column headers and format them\n",
    "    pivot_df.columns = [f'{col[1]}_{col[0]}' for col in pivot_df.columns.values]\n",
    "\n",
    "    pivot_df.reset_index(inplace=True) # Cell is a column used to merge \n",
    "\n",
    "    result_df = pd.merge(dataframe, pivot_df)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NaNs are often placed as default over None. By returning None, it shows that it was on purpose.\n",
    "\n",
    "def Replace_NaNs_With_None(dataframe):\n",
    "    \"\"\"\n",
    "    Replaces all `NaN` values in a DataFrame with `None`, including `NaN` values inside lists and tuples.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame where all `NaN` values have been replaced with `None`.\n",
    "    \"\"\"\n",
    "    \n",
    "    def replace_in_iterable(iterable):\n",
    "        \"\"\"Helper function to replace NaN with None inside a list or tuple.\"\"\"\n",
    "        return type(iterable)(None if pd.isna(item) else item for item in iterable)\n",
    "    \n",
    "    def replace_nans(item):\n",
    "        \"\"\"Replace NaN with None for individual elements, lists, or tuples.\"\"\"\n",
    "        if isinstance(item, (list, tuple)):\n",
    "            return replace_in_iterable(item)\n",
    "        elif pd.isna(item):\n",
    "            return None\n",
    "        else:\n",
    "            return item\n",
    "    \n",
    "    # Apply the replace_nans function to each element in the DataFrame\n",
    "    return dataframe.applymap(replace_nans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cell_Results_Calcs = Cell_Results.copy()\n",
    "\n",
    "# These functions find indices\n",
    "Cell_Results_Calcs = Top_Bottom_Indices_V2(Cell_Results_Calcs)\n",
    "\n",
    "# These functions calculate integrals using the index boundaries\n",
    "Cell_Results_Calcs = TopMidBot_Integrals_V2(Cell_Results_Calcs)\n",
    "Cell_Results_Calcs = Reshape_Integrals(Cell_Results_Calcs)\n",
    "Cell_Results_Calcs = Surface_Integrals_V2(Cell_Results_Calcs)\n",
    "Cell_Results_Calcs = Replace_NaNs_With_None(Cell_Results_Calcs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Making new dataframe for export\n",
    "- Keeps certain columns\n",
    "- Renames columns\n",
    "- Mapping column categories to new names (e.g. eGFP Positive -> 1)\n",
    "'''\n",
    "\n",
    "# Columns to Keep\n",
    "keep_cols = ['DJID', 'Genotype', 'Eye', 'sex', 'Age_Months', 'eGFP_Value', 'Group_Type', 'in_rip', 'Length', 'Time_Condition',\n",
    "             'GluT1_Top_Surface_Integral', 'GluT1_Bot_Surface_Integral', 'WGA_Top_Surface_Integral', 'WGA_Bot_Surface_Integral',\n",
    "             'GluT1_Top_Integral', 'GluT1_Middle_Integral', 'GluT1_Bottom_Integral',\n",
    "             'WGA_Top_Integral', 'WGA_Middle_Integral', 'WGA_Bottom_Integral', 'file_name']\n",
    "\n",
    "# Columns to rename\n",
    "renamed_cols = {'sex': 'Sex',\n",
    "                'Length': 'Length_um',\n",
    "                'in_rip': 'In_Rip',\n",
    "                'Group_Type' :'Experimental_Condition'}\n",
    "\n",
    "Export_df = Cell_Results_Calcs.groupby('Cell').first().loc[:, keep_cols].reset_index(drop = True)\n",
    "Export_df = Export_df.rename(columns = renamed_cols)\n",
    "\n",
    "# Remapping values\n",
    "Export_df['eGFP_Value'] = Export_df['eGFP_Value'].astype(int)\n",
    "Export_df['In_Rip'] = Export_df['In_Rip'].astype(int)\n",
    "\n",
    "Exp_Con_map = {'Experimental': 'Light Flicker', 'Control': 'Dark'}\n",
    "Export_df['Experimental_Condition'] = Export_df['Experimental_Condition'].map(Exp_Con_map)#.fillna(Export_df['Experimental_Condition'])\n",
    "\n",
    "\n",
    "# Raphael 91024. Now invalid since NA values are allowed.\n",
    "# # Checking for NA vals\n",
    "# if Export_df.isna().sum().sum() > 0:\n",
    "#     print('There are NA values, check for problems.')\n",
    "\n",
    "Export_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "Export_df = Export_df.loc[Export_df['Genotype'] != 'wildtype']\n",
    "\n",
    "processed_data_name_csv = #input\n",
    "Export_df.to_csv('processed_data_folder/' + processed_data_name_csv + '.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
