{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Working Directory: d:\\Lab Analysis PC\\ImageAnalysis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Luis\\anaconda3\\envs\\cellpose\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "current_directory = os.getcwd()\n",
    "print(f\"Current Working Directory: {current_directory}\")\n",
    "\n",
    "from Tusc5ImageUtils import *\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from skimage import exposure\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "from cellpose import utils, io, plot, models, denoise\n",
    "from scipy.ndimage import binary_erosion, binary_fill_holes, center_of_mass\n",
    "from scipy.signal import find_peaks\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import csv\n",
    "import re\n",
    "import nd2\n",
    "from skimage.measure import regionprops\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Directory Configuration ###\n",
    "\n",
    "'''\n",
    "Input name of folder with .nd2 files in the parent directory of this python notebook\n",
    "'''\n",
    "\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "target_folder_name = 'one stack'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cell Fluoresence Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    ".nd2 files with flipped top and bottom for whatever reason\n",
    "'''\n",
    "backward_files = ['3604R_GLUT1_WGA_0001.nd2',\n",
    "                  '3604R_GLUT1_WGA_0003.nd2',\n",
    "                  '3654L_GLUT1_WGA_0001.nd2',\n",
    "                  '3654L_GLUT1_WGA_0003.nd2']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Set the min and max for the DAPI projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2007R_GLUT1_647_WGA_594_0002.nd2\n",
      "Processing 3203R_GLUT1_647_WGA_594_0002.nd2\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Code to set min and max for DAPI max projections\n",
    "\n",
    "Automatically records min and max presets in file_mp_configurations.csv\n",
    "If min and max were set multiple times, the most recent parameters are used\n",
    "'''\n",
    "\n",
    "if not parent_directory or not target_folder_name:\n",
    "    raise ValueError('Make sure to input target folder name and parent directory.')\n",
    "\n",
    "mp_configs_df = pd.read_csv('file_mp_configurations.csv')\n",
    "\n",
    "\n",
    "target_folder_path = os.path.join(parent_directory, target_folder_name)\n",
    "nd2_files = [f for f in os.listdir(target_folder_path) if f.endswith('.nd2')]\n",
    "\n",
    "counter = 0\n",
    "\n",
    "for nd2_file in nd2_files:\n",
    "\n",
    "    # 1) Download Image Data\n",
    "    print(f'Processing {nd2_file}')\n",
    "    nd2_path = os.path.join(target_folder_path, nd2_file)\n",
    "\n",
    "    f = nd2.ND2File(nd2_path)\n",
    "    z_sep = f.voxel_size().z\n",
    "    image = to_8bit(f.asarray())\n",
    "    f.close()\n",
    "\n",
    "    # Flip image if image is backwards\n",
    "    if nd2_file in backward_files:\n",
    "        image = np.flip(image, axis=0)\n",
    "\n",
    "    # 2) DAPI max projection, Deblur, Segment\n",
    "    DAPI_stack = image[:, 0, :, :].copy()\n",
    "   \n",
    "    # Check if the nd2_file is already in mp_configs_df\n",
    "    if nd2_file in mp_configs_df['file_name'].values:\n",
    "        # If the file exists in the dataframe, retrieve z0 and z1\n",
    "        file_mp_df = mp_configs_df.loc[mp_configs_df['file_name'] == nd2_file].iloc[-1]\n",
    "        z0, z1 = file_mp_df['z0'], file_mp_df['z1']\n",
    "\n",
    "    else:\n",
    "        # If the file is not in the dataframe, run the max projector app and get z0 and z1\n",
    "        z0, z1 = run_max_projector_app(DAPI_stack)\n",
    "        \n",
    "        # Create the data to write (file_name, z0, z1, date)\n",
    "        mp_config = [nd2_file, z0, z1, datetime.now().strftime(\"%H:%M_%d_%m_%Y\")]\n",
    "        \n",
    "        # Define the CSV file path\n",
    "        csv_file_path = 'file_mp_configurations.csv'\n",
    "        \n",
    "        # Open the CSV file in append mode ('a') with proper newline handling\n",
    "        with open(csv_file_path, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            \n",
    "            # Only add a newline if the file is non-empty and this is the first append\n",
    "\n",
    "            \n",
    "\n",
    "            if os.path.getsize(csv_file_path) > 0 and counter == 0:\n",
    "                f.seek(0, os.SEEK_END)  # Go to the end of the file\n",
    "                f.write('\\n')  # Add a newline to ensure separation between last line and new data\n",
    "                counter += 1\n",
    "\n",
    "            if os.path.getsize(csv_file_path) > 0 and counter > 0:\n",
    "                f.seek(0, os.SEEK_END)  # Go to the end of the file\n",
    "            \n",
    "            # Write the new data\n",
    "            writer.writerow(mp_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2007R_GLUT1_647_WGA_594_0002.nd2\n",
      "Processing 3203R_GLUT1_647_WGA_594_0002.nd2\n",
      "                      file_name       masks_in_rip\n",
      "0  2007R_GLUT1_647_WGA_594_0002       [30, 36, 53]\n",
      "1  3203R_GLUT1_647_WGA_594_0002  [45, 4, 1, 7, 11]\n"
     ]
    }
   ],
   "source": [
    "if not parent_directory or not target_folder_name:\n",
    "    raise ValueError('Make sure to input target folder name and parent directory.')\n",
    "\n",
    "\n",
    "target_folder_path = os.path.join(parent_directory, target_folder_name)\n",
    "nd2_files = [f for f in os.listdir(target_folder_path) if f.endswith('.nd2')]\n",
    "\n",
    "model_path_dapi = os.path.join(parent_directory, 'ImageAnalysis/cellpose_models/T5_DAPI_V4')\n",
    "deblur_model = denoise.CellposeDenoiseModel(gpu=True, model_type= model_path_dapi, restore_type=\"deblur_cyto3\")\n",
    "\n",
    "# Initialize an empty DataFrame with the desired columns\n",
    "rip_df = pd.DataFrame(columns=['file_name', 'masks_in_rip'])\n",
    "\n",
    "for nd2_file in nd2_files:\n",
    "\n",
    "    # 1) Download Image Data\n",
    "    print(f'Processing {nd2_file}')\n",
    "    nd2_path = os.path.join(target_folder_path, nd2_file)\n",
    "\n",
    "    f = nd2.ND2File(nd2_path)\n",
    "    z_sep = f.voxel_size().z\n",
    "    image = to_8bit(f.asarray())\n",
    "    f.close()\n",
    "\n",
    "    # Flip image if image is backwards\n",
    "    if nd2_file in backward_files:\n",
    "        image = np.flip(image, axis=0)\n",
    "\n",
    "    # 2) DAPI max projection, Deblur, Segment\n",
    "    DAPI_stack = image[:, 0, :, :].copy()\n",
    "\n",
    "    mp_DAPI = max_proj(DAPI_stack[5:30].copy())\n",
    "\n",
    "    DAPI_masks, flows, styles, image_deblurred = deblur_model.eval(auto_brightness_contrast(mp_DAPI), diameter=None, channels=[0, 0])\n",
    "    image_deblurred = image_deblurred[:, :, 0]  # resulting image has one channel, but it still needs to be indexed\n",
    "\n",
    "    coords_3d = nuclei_centers_of_mass(DAPI_stack, DAPI_masks)\n",
    "    filtered_coords_3d, filtered_idxs = remove_outliers_local(coords_3d, num_closest_points=15, z_threshold=2)\n",
    "    filtered_DAPI_masks = extract_masks(DAPI_masks, filtered_idxs)\n",
    "    DAPI_masks = filtered_DAPI_masks.copy()\n",
    "\n",
    "    coords_2d = [(i[0], i[1]) for i in filtered_coords_3d]\n",
    "    in_rip_dict = rip_identifier(nd2_file, image, DAPI_masks, coords_2d)\n",
    "    \n",
    "    # Extract file name and masks_in_rip from in_rip_dict (file name is the key, masks_in_rip is the value)\n",
    "    for file_name, masks_in_rip in in_rip_dict.items():\n",
    "        # Append the new data to the DataFrame\n",
    "        new_row = pd.DataFrame({'file_name': [file_name], 'masks_in_rip': [masks_in_rip]})\n",
    "        rip_df = pd.concat([rip_df, new_row], ignore_index=True)\n",
    "\n",
    "# Optionally, display or save the DataFrame at the end of the loop\n",
    "print(rip_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extract Traces\n",
    "\n",
    "- This step takes a folder containing `.nd2` files and returns a DataFrame of traces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 2007R_GLUT1_647_WGA_594_0002.nd2\n",
      "- Masks found: 120/124\n",
      "Processing 3203R_GLUT1_647_WGA_594_0002.nd2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: no mask pixels found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Masks found: 112/118\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "\n",
    "### Setting paths ###\n",
    "if not parent_directory or not target_folder_name:\n",
    "    raise ValueError('Make sure to input target folder name and parent directory.')\n",
    "\n",
    "target_folder_path = os.path.join(parent_directory, target_folder_name)\n",
    "\n",
    "\n",
    "nd2_files = [f for f in os.listdir(target_folder_path) if f.endswith('.nd2')]\n",
    "\n",
    "### Models ###\n",
    "# Setting path for models\n",
    "model_path_dapi = os.path.join(parent_directory, 'ImageAnalysis/cellpose_models/T5_DAPI_V4')\n",
    "model_path_wga = os.path.join(parent_directory, 'ImageAnalysis/cellpose_models/T5_WGA_V2')\n",
    "\n",
    "# Seting the DAPI (deblur) and WGA models\n",
    "deblur_model = denoise.CellposeDenoiseModel(gpu=True, model_type= model_path_dapi, restore_type=\"deblur_cyto3\")\n",
    "wga_model = models.CellposeModel(gpu=True, pretrained_model=model_path_wga)\n",
    "\n",
    "### Reading respective DAPI min and max each respective image stack ###\n",
    "mp_configs_df = pd.read_csv('file_mp_configurations.csv')\n",
    "\n",
    "\n",
    "all_data = pd.DataFrame()\n",
    "unid_counter = 0\n",
    "\n",
    "for nd2_file in nd2_files:\n",
    "\n",
    "    # 1) Download Image Data\n",
    "    print(f'Processing {nd2_file}')\n",
    "    nd2_path = os.path.join(target_folder_path, nd2_file)\n",
    "\n",
    "    f = nd2.ND2File(nd2_path)\n",
    "    z_sep = f.voxel_size().z\n",
    "    image = to_8bit(f.asarray())\n",
    "    f.close()\n",
    "\n",
    "    # Flip image if image is backwards\n",
    "    if nd2_file in backward_files:\n",
    "        image = np.flip(image, axis=0)\n",
    "\n",
    "    # 2) DAPI max projection, Deblur, Segment\n",
    "    DAPI_stack = image[:, 0, :, :].copy()\n",
    "\n",
    "    # Check if the nd2_file is already in mp_configs_df\n",
    "    if nd2_file in mp_configs_df['file_name'].values:\n",
    "        # If the file exists in the dataframe, retrieve z0 and z1\n",
    "        file_mp_df = mp_configs_df.loc[mp_configs_df['file_name'] == nd2_file].iloc[-1]\n",
    "        z0, z1 = file_mp_df['z0'], file_mp_df['z1']\n",
    "\n",
    "    else:\n",
    "        # If the file is not in the dataframe, run the max projector app and get z0 and z1\n",
    "        z0, z1 = run_max_projector_app(DAPI_stack)\n",
    "        \n",
    "        # Create the data to write (file_name, z0, z1, date)\n",
    "        mp_config = [nd2_file, z0, z1, datetime.now().strftime(\"%H:%M_%d_%m_%Y\")]\n",
    "        \n",
    "        # Define the CSV file path\n",
    "        csv_file_path = 'file_mp_configurations.csv'\n",
    "        \n",
    "        # Open the CSV file in append mode ('a') with proper newline handling\n",
    "        with open(csv_file_path, mode='a', newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            \n",
    "            # Only add a newline if the file is non-empty and this is the first append\n",
    "\n",
    "            \n",
    "\n",
    "            if os.path.getsize(csv_file_path) > 0 and counter == 0:\n",
    "                f.seek(0, os.SEEK_END)  # Go to the end of the file\n",
    "                f.write('\\n')  # Add a newline to ensure separation between last line and new data\n",
    "                counter += 1\n",
    "\n",
    "            if os.path.getsize(csv_file_path) > 0 and counter > 0:\n",
    "                f.seek(0, os.SEEK_END)  # Go to the end of the file\n",
    "            \n",
    "            # Write the new data\n",
    "            writer.writerow(mp_config)\n",
    "\n",
    "    mp_DAPI = max_proj(DAPI_stack[z0:z1].copy())\n",
    "\n",
    "    # 3) Deblur and Segment DAPI max projection\n",
    "    DAPI_masks, flows, styles, image_deblurred = deblur_model.eval(auto_brightness_contrast(mp_DAPI), diameter=None, channels=[0, 0])\n",
    "    image_deblurred = image_deblurred[:, :, 0]  # resulting image has one channel, but it still needs to be indexed\n",
    "\n",
    "\n",
    "    # DAPI filtering and eGFP identification\n",
    "    coords_3d = nuclei_centers_of_mass(DAPI_stack, DAPI_masks)\n",
    "    filtered_coords_3d, filtered_idxs = remove_outliers_local(coords_3d, num_closest_points=15, z_threshold=2)\n",
    "    filtered_DAPI_masks = extract_masks(DAPI_masks, filtered_idxs)\n",
    "    DAPI_masks = filtered_DAPI_masks.copy()\n",
    "\n",
    "    ## Quick view\n",
    "    ## plt.imshow(plot.mask_overlay(to_8bit(image_deblurred), DAPI_masks))\n",
    "    ## plt.axis('off')\n",
    "    ## plt.show()\n",
    "\n",
    "    #Identifying cells in rip\n",
    "    ##coords_2d = [(i[0], i[1]) for i in filtered_coords_3d]\n",
    "    ##in_rip_dict = rip_identifier(nd2_file, image, DAPI_masks, coords_2d)\n",
    "\n",
    "    # List for eGFP identification later\n",
    "    eGFP_fluorescence_list = []\n",
    "    \n",
    "    # Initialize a list to accumulate cell data\n",
    "    file_data_list = []\n",
    "\n",
    "    '''\n",
    "    Indiviudal Cell\n",
    "    '''\n",
    "\n",
    "    # 5) Segmentation of WGA channel\n",
    "    mask_idxs = np.delete(np.unique(DAPI_masks), 0) - 1\n",
    "\n",
    "    total_masks = len(mask_idxs)  # Total number of masks to process\n",
    "    masks_found = 0  # Counter for the number of masks found\n",
    "\n",
    "    for mask_id in mask_idxs:\n",
    "\n",
    "        single_mask = extract_masks(DAPI_masks, mask_id)\n",
    "        diam = get_mask_diameter(single_mask)\n",
    "        expansion = 50\n",
    "\n",
    "        sq_stacks = get_sq_stacks(image, single_mask)\n",
    "\n",
    "        # Running the model of the expanded squares\n",
    "        expanded_sq_WGA, z_level = extract_square_proj_expand(image, single_mask, expansion)\n",
    "\n",
    "        expanded_mask, flows, styles = wga_model.eval(expanded_sq_WGA, diameter=diam, channels=[0, 0])\n",
    "\n",
    "        # Removing 0-pixel boundary and finding the largest mask in the array\n",
    "        WGA_mask = remove_boundary(expanded_mask, expansion)\n",
    "\n",
    "        if len(np.unique(WGA_mask)) == 1:\n",
    "            continue\n",
    "\n",
    "        elif len(np.unique(WGA_mask)) > 2:\n",
    "            WGA_mask = closest_mask_2d(single_mask, WGA_mask)\n",
    "\n",
    "        masks_found += 1  # Increment the masks found counter\n",
    "\n",
    "        # Z-axis profile\n",
    "        trace_results = get_traces(sq_stacks, WGA_mask)\n",
    "\n",
    "        # eGFP extraction\n",
    "        eGFP_sum = np.sum(sq_stacks[1, z_level, :, :][(WGA_mask.astype(bool))])\n",
    "        eGFP_sum_per_area = eGFP_sum / np.sum(WGA_mask)\n",
    "\n",
    "        eGFP_fluorescence_list.append((mask_id, eGFP_sum_per_area))\n",
    "\n",
    "        '''\n",
    "        Organizing data\n",
    "        '''\n",
    "\n",
    "        # 6) Converting trace results into a pd dataframe\n",
    "        cell_data = organize_data(trace_results, mask_id)\n",
    "\n",
    "        # 7) Adding file information\n",
    "        djid, eye, file_base = extract_information(nd2_file)\n",
    "\n",
    "        nested_array = np.array(range(image.shape[0])) * z_sep\n",
    "        cell_data['X_vals'] = [nested_array for i in range(len(cell_data))]\n",
    "        cell_data['file_name'] = file_base\n",
    "        cell_data['DJID'] = djid\n",
    "        cell_data['Eye'] = eye\n",
    "        cell_data['eGFP_Value'] = False\n",
    "        cell_data['eGFP_Raw_Intensity'] = eGFP_sum_per_area\n",
    "\n",
    "        # Adding in_rip information\n",
    "        cell_data['in_rip'] = False\n",
    "\n",
    "        ##for file_name, mask_ids in in_rip_dict.items():\n",
    "        ##    if file_name == file_base and mask_id in mask_ids:\n",
    "        ##        cell_data['in_rip'] = True\n",
    "\n",
    "        # Accumulating cell data\n",
    "        file_data_list.append(cell_data)\n",
    "\n",
    "    # Print the number of masks found out of the total possible masks\n",
    "    print(f'- Masks found: {masks_found}/{total_masks}')\n",
    "\n",
    "    # Concatenating the list into a single DataFrame for the file\n",
    "    file_data = pd.concat(file_data_list, ignore_index=True)\n",
    "\n",
    "    # Processing eGFP data for the entire file\n",
    "    eGFP_idxs = np.array([i[0] for i in eGFP_fluorescence_list])\n",
    "    eGFP_vals = np.array([i[1] for i in eGFP_fluorescence_list])\n",
    "\n",
    "    # Normalizing eGFP values\n",
    "    eGFP_vals_normal = normalize(eGFP_vals)\n",
    "\n",
    "    # Setting cells above .2 as positive\n",
    "    eGFP_pos_idxs = eGFP_idxs[eGFP_vals_normal > .2]\n",
    "    file_data.loc[file_data['mask_id'].isin(eGFP_pos_idxs), 'eGFP_Value'] = True\n",
    "\n",
    "    # Accumulating data at the all_data level\n",
    "    all_data = pd.concat([all_data, file_data], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to \n",
    "\n",
    "def merge_rip_ids(df_rip, df_masks):\n",
    "    \"\"\"\n",
    "    Merge two DataFrames based on matching file_name and mask values.\n",
    "\n",
    "    Parameters:\n",
    "    df_rip (pd.DataFrame): DataFrame containing 'file_name' and 'masks_in_rip' (list of masks).\n",
    "    df_masks (pd.DataFrame): DataFrame containing 'file_name' and 'masks' (single mask values).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: The merged DataFrame with an 'in_rip' column indicating if the mask is in masks_in_rip.\n",
    "    \"\"\"\n",
    "    # Create a copy of df_masks to avoid modifying the original DataFrame\n",
    "    df_result = df_masks.copy()\n",
    "    \n",
    "    # Initialize 'in_rip' column to False\n",
    "    df_result['in_rip'] = False\n",
    "    \n",
    "    # Iterate through df_rip and check for matches\n",
    "    for i, row in df_rip.iterrows():\n",
    "        file_name = row['file_name']\n",
    "        masks_in_rip = row['masks_in_rip']\n",
    "        \n",
    "        # For matching file_name, check if any masks match\n",
    "        mask_condition = (df_result['file_name'] == file_name) & (df_result['mask_id'].isin(masks_in_rip))\n",
    "        \n",
    "        # Set 'in_rip' to True where matches are found\n",
    "        df_result.loc[mask_condition, 'in_rip'] = True\n",
    "    \n",
    "    return df_result\n",
    "\n",
    "all_data = merge_rip_ids(rip_df, all_data).query('in_rip == True')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df = all_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code for quick examination of any single cell\n",
    "##export_df_temp = export_df.copy()\n",
    "##export_df_temp['Cell_unid'] = export_df.groupby(['file_name', 'mask_id']).ngroup()\n",
    "##plot_single_cell(export_df_temp.query('Cell_unid == 10'), prominence =15, distance = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "export_df['X_vals'] = export_df['X_vals'].apply(lambda x: ', '.join(map(str, x)))\n",
    "export_df['Y_vals'] = export_df['Y_vals'].apply(lambda x: ', '.join(map(str, x)))\n",
    "export_df = export_df.loc[export_df['file_name'] != '3607R_GLUT1_WGA_0001']\n",
    "\n",
    "### VVV ATTENTION VVV\n",
    "\n",
    "raw_data_csv_name = #INPUT\n",
    "export_df.to_csv('raw_data_folder/' + raw_data_csv_name + '.csv', index = False) # Carefully modify export csv name"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
